{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< The number of words , The number of dimensionality >: < 17 , 2 >\n",
      "len of word_vectors_dic: 17\n",
      "=== contents lines ===\n",
      "['I am a boy your are a girl\\n', 'dancing now what time is it\\n']\n",
      "=== duplication of word ===\n",
      "The usage of duplication of word is permitted\n",
      "=== Before sum of vectors ===\n",
      "Contents line: I am a boy your are a girl\n",
      "The resuling split(): ['I', 'am', 'a', 'boy', 'your']\n",
      "The sum of a line( 0 ): [ 0.675  0.775]\n",
      "=== Sequence words ===\n",
      "[(0.99999738178428776, 'boy'), (0.999867590697748, 'your'), (0.99976231533627713, 'a'), (0.99967729525693816, 'are'), (0.99949326546593664, 'girl')]\n",
      "A sequence of words: boy your a are girl\n",
      "=== Random words ===\n",
      "It will be shuffled!\n",
      "[(0.99945881852019247, ['dancing', 'your', 'boy', 'are', 'it'])]\n",
      "A random sequence of words: dancing your boy are it\n",
      "\n",
      "=== duplication of word ===\n",
      "The usage of duplication of word is permitted\n",
      "=== Before sum of vectors ===\n",
      "Contents line: dancing now what time is it\n",
      "The resuling split(): ['dancing', 'now', 'what', 'time', 'is']\n",
      "The sum of a line( 1 ): [ 2.   2.1]\n",
      "=== Sequence words ===\n",
      "[(0.99999935726431133, 'time'), (0.99999921875091557, 'what'), (0.99999515765986913, 'is'), (0.99999127113174113, 'now'), (0.99998857544740216, 'it')]\n",
      "A sequence of words: time what is now it\n",
      "=== Random words ===\n",
      "It will be shuffled!\n",
      "[(0.999995706968833, ['one', 'your', 'the', 'am', 'now'])]\n",
      "A random sequence of words: one your the am now\n",
      "\n",
      "=== duplication of word ===\n",
      "The usage of duplication of word is permitted\n",
      "=== Before sum of vectors ===\n",
      "Contents line: which one is the best\n",
      "The resuling split(): ['which', 'one', 'is', 'the', 'best']\n",
      "The sum of a line( 2 ): [ 2.86  2.96]\n",
      "=== Sequence words ===\n",
      "[(0.9999999728764174, 'one'), (0.99999950062478882, 'which'), (0.99999914357764896, 'the'), (0.99999745483820768, 'best'), (0.99999705995364307, 'it')]\n",
      "A sequence of words: one which the best it\n",
      "=== Random words ===\n",
      "It will be shuffled!\n",
      "[(0.99992738761012878, ['I', 'now', 'one', 'are', 'it'])]\n",
      "A random sequence of words: I now one are it\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Author: hyunyoung2(github Id)\n",
    "# To compare sentence vector with word vector to find out title search \n",
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Where the dataset such as contents_normalzed, title \n",
    "ROOT = \"./practice\" # \"../dataset\"\n",
    "\n",
    "\n",
    "# The path of files\n",
    "# contents, title_label, title_predict(current directory), word_vectors\n",
    "PATHS = [\"contents_normalized\", \"title\", \"title_predicted\", \"word_vectors\"]\n",
    "\n",
    "# content literally contains a doc into a line \n",
    "# I like it which is ..\\n\n",
    "# dancing is good ...\\n\n",
    "CONTENTS = os.path.join(ROOT, PATHS[0])\n",
    "\n",
    "# titlel literally contains title of a doc\n",
    "# taste \n",
    "# Korea \\t times\n",
    "# delimiter is \\t between words\n",
    "TITLE_LABEL = os.path.join(ROOT, PATHS[1])\n",
    "\n",
    "# title_predicted literally the words we predict as title \n",
    "# delimiter is \\t between words\n",
    "TITLE_PREDICTED = os.path.join(ROOT, PATHS[2])\n",
    "\n",
    "# word_vectors literally word vectors. \n",
    "# The first line olny include -\n",
    "# a pair of <the number of words without duplication, the number of dimensionality>\n",
    "# delimiter is \\t between words\n",
    "WORD_VECTORS = os.path.join(ROOT, PATHS[3])\n",
    "\n",
    "# split function the standard is \\t, \\n, and white space\n",
    "def compat_splitting(line):\n",
    "    return line.split()\n",
    "\n",
    "# compare similarity between two word with consine\n",
    "def similarity(v1, v2):\n",
    "    n1 = np.linalg.norm(v1)\n",
    "    n2 = np.linalg.norm(v2)\n",
    "    return np.dot(v1, v2) / n1 / n2\n",
    "\n",
    "\n",
    "# read word_vector file \n",
    "word_vectors_dic = dict()\n",
    "num_word_vectors = 0\n",
    "dimension_word_vectors = 0\n",
    "with open(WORD_VECTORS, \"r\") as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx == 0: \n",
    "            tab = compat_splitting(line)\n",
    "            num_word_vectors = tab[0]\n",
    "            dimension_word_vectors = tab[1]\n",
    "            print(\"< The number of words , The number of dimensionality >:\", \n",
    "                  \"<\", num_word_vectors, \",\" , dimension_word_vectors, \">\")\n",
    "            \n",
    "            continue\n",
    "        try :\n",
    "            #print(\"\\nline : before splitting line[\", idx, \"]\", line)\n",
    "            tab = compat_splitting(line)\n",
    "            #print(\"tat : after splitting line[\", idx, \"]\", tab)\n",
    "            vec = np.array(tab[1:], dtype=np.float64) # the type of data is float64\n",
    "            #print(\"vec : after splitting line[\", idx, \"]\", vec, type(vec), vec.dtype)\n",
    "            word = tab[0]\n",
    "            #print(\"word_vectors_dic(tab[0]) : after splitting line[\", idx, \"]\", word)\n",
    "            if np.linalg.norm(vec) == 0:\n",
    "                print(\"np.linalg.norm(vec) is zero\")\n",
    "                continue\n",
    "            if not word in word_vectors_dic:\n",
    "                word_vectors_dic[word] = vec\n",
    "        except ValueError:\n",
    "            print(\"ValueError happens\")\n",
    "            continue\n",
    "        except UnicodeDecodeError:\n",
    "            print(\"Unicode DecodeError\")\n",
    "            continue\n",
    "#  To check the length of the word_vectors_dic\n",
    "print(\"len of word_vectors_dic:\", len(word_vectors_dic))\n",
    "\n",
    "\n",
    "\n",
    "# To get the n word vectors similar with sum_arr in a sequence of word_vectors_dic\n",
    "def how_similar_sequantially(sum_arr, n):\n",
    "    # sum_arr : the sum of a line vector\n",
    "    # n : retrun size. \n",
    "    # prediction : list contins paris of (similarity of consine, word)\n",
    "    prediction = list()\n",
    "    for vector_word, vector_value in word_vectors_dic.items():\n",
    "        prediction.append((similarity(sum_arr, vector_value), vector_word))\n",
    "    prediction.sort(reverse = True)\n",
    "    return prediction[0:5]\n",
    "    \n",
    "# to get n pair of word vectors similiar with sum_arr between in shuffle and in no-shuflle.\n",
    "def how_similar_randomly(sum_arr, n , shuffle=True, return_size=1):\n",
    "    # sum_arr : the sum of a line vector\n",
    "    # n : retrun size. \n",
    "    # prediction : list contins paris of (similarity of consine, word)\n",
    "    # word_list : keys of word_vectors_dic.keys()\n",
    "    # paris_words : pairs of n size in word_list\n",
    "    prediction = list()\n",
    "    word_list = list(word_vectors_dic.keys())\n",
    "    \n",
    "    ##print(\"word_list befer shuffling:\", word_list)\n",
    "    \n",
    "    # To shuffle words\n",
    "    if shuffle == True:\n",
    "        print(\"It will be shuffled!\")\n",
    "        random.shuffle(word_list)\n",
    "    else: \n",
    "        print(\"It will not be shuffled!\")\n",
    "        \n",
    "    ##print(\"word_list after shuffling:\", word_list)\n",
    "    \n",
    "    # to pack n size from words. \n",
    "    pairs_words = [word_list[x:x+n] for x in range(0, len(word_list), n)]\n",
    "    \n",
    "    for pair_idx, pair_value in enumerate(pairs_words):\n",
    "        ##print(pair_value, type(pair_value))\n",
    "        # if the pair_value is less than n, I will not deal with it\n",
    "        if len(pair_value) < n: \n",
    "            continue\n",
    "        sum_pair = np.zeros(int(dimension_word_vectors), dtype=np.float64)\n",
    "        \n",
    "        for idx, val in enumerate(pair_value):\n",
    "            ##print(val, end=\" \")\n",
    "            sum_pair += (word_vectors_dic[val]/n)\n",
    "        ##print(sum_pair)\n",
    "        prediction.append((similarity(sum_arr, sum_pair), pair_value))\n",
    "    prediction.sort(reverse = True)\n",
    "    return prediction[0:return_size]\n",
    "        \n",
    "\n",
    "# for contents_normalized file\n",
    "with open(CONTENTS, \"r\") as f:\n",
    "    contents_lines = [x for x in f.readlines()]\n",
    "    if \"\\n\" in contents_lines:\n",
    "        print(\"contents_lines error!!!\")\n",
    "\n",
    "print(\"=== contents lines ===\")\n",
    "print(contents_lines[0:2])\n",
    "\n",
    "# To store a sequence of title we sequentailly predict\n",
    "SEQUENCDE_PREDICTED = os.path.join(os.getcwd(), \"sequence_predicted\")\n",
    "##print(SEQUENCDE)\n",
    "\n",
    "# To store a sequence of title we randomly predict\n",
    "RANDOM_PREDICTED = os.path.join(os.getcwd(), \"random_predicted\")\n",
    "##print(RANDOM)\n",
    "\n",
    "\n",
    "duplication_of_word = True\n",
    "size_extracted = 5\n",
    "\n",
    "with open(SEQUENCDE_PREDICTED, \"w\") as ws:\n",
    "    with open(RANDOM_PREDICTED, \"w\") as wr:\n",
    "        # how to calculate the sum of word vector in a line\n",
    "        for idx, line in enumerate(contents_lines):\n",
    "            line_words = list()\n",
    "            if duplication_of_word == True:\n",
    "                line_words = compat_splitting(line)\n",
    "            else:\n",
    "                for counter_idx, counter_val in enumerate(collections.Counter(line.split()).most_common()):\n",
    "                    line_words.append(counter_val[0])\n",
    "            \n",
    "            print(\"=== duplication of word ===\")\n",
    "            if duplication_of_word == True:\n",
    "                print(\"The usage of duplication of word is permitted\")\n",
    "            else:\n",
    "                print(\"The usage of duplication of word is not permitted\")\n",
    "                \n",
    "            print(\"=== Before sum of vectors ===\")\n",
    "            print(\"Contents line:\", line, end=\"\")\n",
    "            print(\"The resuling split():\", line_words[0:5])\n",
    "\n",
    "            # sum vector of a line\n",
    "            sum_numarray = np.zeros((int(dimension_word_vectors)), dtype=np.float64)\n",
    "\n",
    "            # current line.split includes duplication of words. \n",
    "            # if you want no duplicatino use the following, use duplication_of_word = False\n",
    "            # the sum of word vectore in a line, the sum is average.\n",
    "            for words in line_words:\n",
    "                sum_numarray += (word_vectors_dic[words]/len(line_words))\n",
    "\n",
    "            # from here on, I have to retrieve the 5 words to be similar with **sum_numarray**\n",
    "            print(\"The sum of a line(\",idx,\"):\", sum_numarray)\n",
    "            \n",
    "            print(\"=== Sequence words ===\")  \n",
    "            sequence_words = how_similar_sequantially(sum_numarray, size_extracted)\n",
    "               \n",
    "            print(sequence_words)\n",
    "            print(\"A sequence of words:\", end=\" \")\n",
    "            for seq_id, seq_word in enumerate(sequence_words):\n",
    "                if seq_id < size_extracted - 1:\n",
    "                    print(seq_word[1], end=\" \")\n",
    "                    ws.write(seq_word[1]+\"\\t\")\n",
    "                elif seq_id == (size_extracted - 1):\n",
    "                    print(seq_word[1], end=\"\\n\")\n",
    "                    ws.write(seq_word[1]+\"\\n\")\n",
    "                    \n",
    "            print(\"=== Random words ===\")\n",
    "            #print(how_similar_sequantially(sum_numarray, 5))\n",
    "            radom_words = how_similar_randomly(sum_numarray, size_extracted)\n",
    "            \n",
    "            print(radom_words)\n",
    "           \n",
    "            for ran_id, ran_word in radom_words:\n",
    "                print(\"A random sequence of words:\", end=\" \")\n",
    "                for ran_word_idx in range(size_extracted):\n",
    "                    if ran_word_idx < size_extracted - 1:\n",
    "                        print(ran_word[ran_word_idx], end = \" \")\n",
    "                        wr.write(ran_word[ran_word_idx]+\"\\t\")\n",
    "                    elif ran_word_idx == size_extracted - 1:\n",
    "                        print(ran_word[ran_word_idx])\n",
    "                        wr.write(ran_word[ran_word_idx]+\"\\n\")\n",
    "            \n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo/v:0\n",
      "foo/v\n",
      "foo/bar/add:0\n",
      "foo/bar/add\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    with tf.name_scope(\"bar\"):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "        x = 1.0 + v\n",
    "\n",
    "print(v.name)\n",
    "print(v.op.name)\n",
    "print(x.name)\n",
    "print(x.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar/v:0\n",
      "bar/v\n",
      "foo_1/bar/add:0\n",
      "foo_1/bar/add\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"foo\"):\n",
    "    with tf.variable_scope(\"bar\"):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "        x = 1.0 + v\n",
    "\n",
    "print(v.name)\n",
    "print(v.op.name)\n",
    "print(x.name)\n",
    "print(x.op.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
